Random Forest Implementation (Ensembling Method)
------------------------------------------------
Collection of Trees (Forest)


A single Decision Tree is high variance model and low bias

To rectify this problem we are using Random Forest


Let X1, X2, ... Xn be random variables
which are independent and identically distributed (iid)

In sampling distribution
if 	X -> Sigma
	X^ -> Signma/sqrt(n)
	
similarly
Var(Xi) = Sigma**2
Var(Xi^) = (Sigma**2)/n

If we drop the independent assumption 
 Var(X^) = Row*Sigma**2 + ((1-Row)/n)*Sigma**2
 
 Row -> Correlation between Xi's
 n -> Number of models
 
 Minimize the Var(X^)
 
 1. Increase n then Var(X^) will go down
 2. We want models to be as decorrelated as possible

So points 1 & 2 are striving ot reduce Var(Xi^) where
 n-> No.of models
 Row - Corr between models



Different ways to Ensemble

1. Use different Alogs & Stack them
	RF, SVM, NN
	
2. Bagging - Create approximately different training sets from same dataset (Random Forest)
3. Boosting - Sequential training method (Ada Boost, XG Boost)
				produces boosted DT
				

Bagging
-------

Bootstrap and Aggregration (Bagging)

1. Bootstrap
	Statistical method 
	
	if true population is unknown,
	We have a sample S
	
	Assumption: P ~ S
	
	Draws repeated samples with replacement from S to get N Bootstrap samples
	N Samples are called Bags
	
	Bag1, Bag2 .. Bagn
	
	2/3*S or 66%
	
	For each Bag we have DT1, ... DTn
	All of them are trained in parallel
	
	n is a hyperparameter -> n_estimators (higher the better)

2. Aggregation of the results
	DT1, DT2, ... , DTn (Trees)
	y1, y2, ... , yn (Predictions)
	
	if yi are numerical we aggregate by taking the mean
	y^ = Avg(y1^ + ...+ yn^)
	
	if yi^ are categorical
	y^ = Mode(y1^ + ...+ yn^)  -- Majority Vote
	
	RF = Bagged Decision Trees
	
	Best part of RF is no overfitting
	
	RF decorrelates the trees further by taking at each split only a random sample of features 
	if P=9 (Features)
	it chooses sqrt(9) i.e. 3 random features
	
							R1 (X1,X3,S7)
							/		\
						X4,X5,X6	X2,X6,X7
	This aspect is controlled by hyperparameter max_features
	
	C => Actual = Pred  I=> Actual != Pred
	Acc	->	0.7		0.7		0.7
	Case	DT1 	DT2		DT23	Result	Prob
	1		C		C		C		C		0.7*0.7*0.7
	2		C		C		I		C		0.7*0.7*0.3
	3		C		I		C		C
	4		I		C		C		C
	5		I		I		C		I
	6		I		C		I		I
	7		C		I		I		I
	8		I		I		I		I
	
	P(ensemble is correct) = Add(C+C+C+C) = 0.784 which is > 0.7
	Aggregation always improves performance
	
Summary of Random Forest
1. We choose the number of trees i.e. model by n_estimators e.g 200
2. RF creates 200 Boostrap samples 
3. Fit 200 DT to the BSS
	uses random subset of features at each step
4. Get the prediction accuracy enhanced


Pros and Cons of Random Forest

Pros
----
	1. Produces a low variance ensemble model 
	2. Immune to the curse of dimensionality(no.of features), decorrelates the trees by taking subset of features at each split
	3. Very robust towards imbalanced datasets
	4. Feature importances
	
Cons
-----
	1. OOB(Out of Bag error) aka cross validation error 
	2. No visualization of the model
	
	
RF uses 2/3 of the samples 
remaining 1/3 is OOB Error

all the oob's are combined to form a OOB bag

Sample X1 may not be in DT1 but may be present in DT2, so X1 is predicted in DT2
Aggregate all e1, e2 ... en for getting OOB error
The Data which was not used in original tree, we need to get what error it would produce with other tree
	