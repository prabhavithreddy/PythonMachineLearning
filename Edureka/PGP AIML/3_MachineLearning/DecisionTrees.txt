Decision Trees
--------------
Example (Having party on 16th Aug)
												|
												|
									Is 16th is a holiday
									/Y					\N
							Do we have budget		Reschedule
							/Y			\N
						Organize		Reschedule

	Set of Decision Rules (if else statements)

	Which question we need to ask to take first decision?
		Budget could be the first according to our understanding	
		Order of question is based on priority
		
	1. There should be some mathematical basics at the arriving of the order of questions  -> Decision Rules
	2. Order of question can affect the time taken to make a decision - Efficiency
		Most important question first then next
	3. We need historical labelled data - Supervised Data
	4. This kind of rule based engine is called Decision Tree
	5. We can explain the model - Visualize

	Geometrically (R1, R2, R3 -> Regions)
	Example: Employee leaving a company or not
	
	
	X2
	^ R1
	| x	|		x		(y=0)
	|x	|	x	R2	x
	| x	|_____________
	|	|	x x	R3		(y=1)
	|	|
	|	|		 x
	------------------>X1
		 X1
	N-dimentional predictor space
	
	if x < x1 then y=0
	
	Question is which Xn has to be choosen (which split should be taken first)
	
1. Decision Trees create rectangular regions in predictors space using parition lines that are parallel to axis
2. For 3 dimentional we use partition planes
3. Works for both regression and classification
4. Find the region where new point is going to fit in
5. Gives the median/average value of Region 


How does a Tree Splitting works

	1. R1 = {x<4.5} 		-> y^ = 165K
	2. R2 = {x>4.5 & y<118} -> y^ = 402K
	3. R3 = {{x>4.5 & y>118}-> y^ = 845K
	
The decision is made by minimizing loss function
The loss function is RSS(Residual Sum Squares)
J
E	E  (yi - y^j)
j=1 i->Rj

How does a classification decision tree works
1. Entropy (Randomness)
	Means the measure of randomness of data
	Measure of impurity of the data
		e.g. 	Bag A [4 Red, 5 Blue] -> P(Red) = 4/9
				Bag B [7 Red, 1 Blue, 1 White] -> P(Red) = 7/9
				
				Bag A is impure since its probability is lower compared to Bag B
				Bag B has less entropy compared to Bag A
		if probability is 1/0 then it is pure
	if entropy is high then impurity is high (P~0.5)
	if entropy is low then impurity is low (P~ 1|0)
	
	E(x) 	= -Sigma Pilog2(Pi)
			= -Plog2(P) - Qlog2(Q)
			= All balls are red/ If all balls are not red
			= -1 log2(1) - 0log2(0)
			= 0
	if P = 0.5 then E(x) = 1
	
	Graph
 (1)|   ---
	|  /   \
	| /     \
	|/_______\__
	0	0.5  1
	
	Goal is to minimize the entropy 
	Reduce Entropy to 0 by maximizing IG
	
	--- E1
	/ \	
   -- -- E2
   /   \
   
   E1 - E2 = Information Gain
   
	
2. Gini Index

	Measure of impurity/randomness
	Xi, J-> No.of classes
	
			J
	G = 1 - E	Pi2
			i=1
			
	e.g. P= 0.5, Q = 0.5
	
	G 	= 1-(P**2 + Q**2)
		= 1 - (0.5**2 + 0.5**2)
		= 1 - (0.25 + 0.25)
		= 0.5
	
	e.g. P=0, Q=1 then G = 0
	
			Graph
   (0.5)|   ---
		|  /   \
		| /     \
		|/_______\__
		0	0.5  1

	
	Mathematically G & E are similar
	G is computationally more efficient since it is just square
	sklearn uses G as default (Criterion)
	
3. Information Gain
	
	It measures how good or bad any split will be based on impurity measure we have choosen
	Root node is decided on information gain

	Leaf nodes where decision are made
	
	IG = E1 - E2
	IG = E(T) - E(T,X) where T is parent node, X is child node
	IG inversly propotional to E
	
	Difference between entropy of upper node and lower node
	
	Decision Tree looks to maximize the IG
	
	GOAL -> Maximize IG
	
	
Instructor explained an example using Excel


Implementation of Decision Tree
1. ID3 (Developed by Ross Quinlan) - Iterative Dichotomizer 3
	Greedy approach at each node
	check one node at a time
	Made for classification 
	Only Categorical Feature (e.g. binning a salary <40, 40<&<60 ...)
	
2. C4.5 (Classification)
	Overcame the limitation of features to be categorical
	Convert the trained trees to be a set of if-then rules
	
3. CART (Classification and Regression Tree)
	Support numerical target variable
	scikit learn uses CART
	Good for both classification and regression 
	
Advantages of DT
1. Very easy to interpret
2. Very visual model we could display a tree
3. Scaling is optional 
4. Solve both classification and regression

Cons
1. Very prone to overfitting (poorly generalize)
2. Easily affected by outliers (efficiency and computation time)

Solution of overfitting is Pruning (trim)
Hyperparameter(Max Depth) 

1. max_depth
2. min_samples_leaf (final node)
	- minimum no.of samples to allow in each leaf
		e.g. if it is 5 then it will not split beyond 5
3. min_samples_split 
	- minimum no.of samples required to split individual node(branch node)
	e.g. 20 branch node should have atlease 20 items


Instructor started Lab
Employee Attrition

How does a Tree Splitting Works
It take a top down, greedy approach that is known as recursive binday splitting.
It is greedy because at each step of the tree building process, the best split is made at that particular step, rather than
looking ahead and picking a split that will lead to a better tree in some future step.

Implementation steps
	1. Calculate entropy for each branch or split
	2. Split the dataset by selecting attribute with highest information gain
	3. Repeat the above steps on each branch
	4. A branch with entropy of zero is leaf node
	
	
Decision tree: Gini, Entropy, Misclassification Error (Not used anymore)

Pruning
	1. Decision tree tends to get overfit with training sample and becomes too large and complex.
	2. Pruning may be defined as shorteninng the branches of tree
	3. Pruning is done by controlling the hyperparameter called max depth
	
Hyperparameter of DT
Maximum Depth
Minimum number of samples per leaf
Minimum sample split